{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "## Decision Trees and Random Forests for Regression, Part 2\n",
    "\n",
    "### About this notebook\n",
    "\n",
    "The general description and instructions as well as questions for the work with Part 2 of the assignment (this notebook) are found in the Assignment description in Canvas!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOU DON'T HAVE TO RUN THIS IF EVERYTHING IS ALREADY INSTALLED CORRECTLY\n",
    "!pip3 install --upgrade pip\n",
    "!pip3 install graphviz\n",
    "!pip3 install dtreeviz\n",
    "!pip3 install numpy scipy\n",
    "\n",
    "!pip3 install ordered_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Dataset(s)\n",
    "\n",
    "First load the dataset you want to use. Ultimately, you should be working with the **California housing data**, but for quicker test runs, it might help to first start out with the **Diabetes data**. For an initial test that your implementation actually works correctly, you should use the **ConceptData**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split \n",
    "import graphviz\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "test_case = 'concept'\n",
    "#test_case = 'california'\n",
    "#test_case = 'diabetes'\n",
    "\n",
    "if test_case == 'california':\n",
    "    dataset = fetch_california_housing()\n",
    "elif test_case == 'diabetes':\n",
    "    dataset = load_diabetes()\n",
    "elif test_case == 'concept':\n",
    "    print(\"running on toy data, actual data will be loaded later\")\n",
    "else:\n",
    "    raise ValueError('Unknown test case')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using real data, split the set into train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (test_case == 'california') or (test_case == 'diabetes'):\n",
    "    X = dataset.data\n",
    "    y = dataset.target\n",
    "\n",
    "    train_ratio = 0.70\n",
    "    validation_ratio = 0.15\n",
    "    test_ratio = 0.15\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - train_ratio, random_state=0)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 1-2: Creating and evaluating an ID3-based Regression Tree\n",
    "\n",
    "The following cells provide some framework for creating / testing your own, ID3-based, regressor. To see that your tree is constructed correctly, some prints are provided (essentially from the lecture) within the handout directory, that are created with the \"ConceptData\" from the lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1:\n",
    "To make experiments with categorical data, you need to discretize - bin - the data (this goes both for the 'california' and the 'diabetes' cases, for the toy data - 'concept' - case, this is naturally given, note that the data are only loaded below). \n",
    "In order to have the entire data set \"as is\" for the binning, you can prepare a binning rule on the original data (X), that you then apply to your train and test data sets. To make sure that you do not miss any possible attribute values, use the entire set (X) again when providing the categorical values (here bin indices) to the ID3 tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ConceptDataRegr import ConceptDataRegr\n",
    "import ConceptDataRegr as cd\n",
    "\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "import numpy as np\n",
    "\n",
    "# For testing that you get the correct output from the ID3 / your score implementation, \n",
    "# you can use the \"ConceptData\" data set (toggle in step 0!):\n",
    "if test_case == 'concept' :\n",
    "    attributes, binned_X_train, y_train, binned_X_test, y_test = cd.ConceptDataRegr().get_data()\n",
    "    binned_X_val = []\n",
    "    y_val = []\n",
    "     \n",
    "else :\n",
    "    # you might want to try different numbers of bins to see whether using a multivalued branching actually can \n",
    "    # improve results over binary decisions only\n",
    "    \n",
    "    # ΝΟΤΕ: it is NOT necessary to use the same number of bins for all parameters, e.g. [3,11,4,2,2,7,8,3]\n",
    "    # could be a really good choice for the California data - or not (no idea if it is, this is just an EXAMPLE)\n",
    "    \n",
    "    # HINT: inspect the data set description in part 1 to find suitable numbers - there is, for example, \n",
    "    # one feature in the Diabetes data that is binary by nature, so it does not make sense to use anything \n",
    "    # else but two bins for that feature\n",
    "    \n",
    "    if test_case == 'california':\n",
    "        bins = [2,2,2,2,2,2,2,2]\n",
    "    elif test_case == 'diabetes':\n",
    "        bins = [2,2,2,2,2,2,2,2,2,2]\n",
    "        \n",
    "    # here you can test to use different strategies, see the KBinsDiscretizer documentation\n",
    "    binner = KBinsDiscretizer(n_bins=bins, encode='ordinal', strategy='kmeans')\n",
    "    binning_rule = binner.fit(X)\n",
    "    # use the binning rule\n",
    "    binned_X = binning_rule.transform(X).astype(int)\n",
    "    binned_X_train = binning_rule.transform(X_train).astype(int)\n",
    "    binned_X_val = binning_rule.transform(X_val).astype(int)\n",
    "    binned_X_test = binning_rule.transform(X_test).astype(int)\n",
    "\n",
    "    # create an attribute list with \"conceptual\" features/attributes\n",
    "    attributes = {}\n",
    "    i = 0\n",
    "    for attr in dataset.feature_names :\n",
    "        attributes[attr] = set(binned_X[:,i])\n",
    "        i+=1\n",
    "\n",
    "print(attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "Set up and fit the tree, create the dot-data for visualisation. Fill the stubb for *id3_score* below and compare results against the sklearn implementation using different parameters for *max_depth* and *min_samples_leaf*, as well as (many) different binnings. \n",
    "\n",
    "Evaluate (on the **California housing data**) two different parameter settings for *max_depth* and *min_samples_leaf* each as well as *two different binnings* (where one should be a binary binning for direct comparison with the SKLearn tree model) in a structured way (e.g. produce a table with the different combinations of parameters). Compare against the **respective same parameter settings** (where possible) for regressor1 from Part 1. \n",
    "Hint: there are binnings (non-uniform such) for which the ID3 actually outperforms the CART implementation in SciKitLearn.\n",
    "\n",
    "Do **not** try to compare to the random forest score from Part 1, that will be the next step!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- IMPLEMENTATION TASK BELOW THIS LINE ------------------------  \n",
    "import ID3_reg\n",
    "\n",
    "# As of now, the ID3_reg class does not provide any score-method - please replace the stubb below by\n",
    "# a proper method according to the description of DecisionTreeRegressor.score() for easier \n",
    "# comparison with the scikit-learn trees!\n",
    "    \n",
    "def id3_score(predicted, target) :\n",
    "    score = 1.0\n",
    "        \n",
    "        # ************************************************\n",
    "        # Implement your score method here\n",
    "        # ************************************************\n",
    "        \n",
    "    return score\n",
    "\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Now, set up the tree (inspect the ID3_reg class to see the different parameter options, start out with the\n",
    "# \"Concept Data\" from the lecture)    \n",
    "id3 = ID3_reg.ID3RegressionTreePredictor()\n",
    "\n",
    "# Note: the implementation needs both the complete dictionary over all attributes plus the list of \n",
    "# attributes (names only) that are actually to be used for the particular run of fit()\n",
    "myTree = id3.fit(binned_X_train, y_train, attributes, attributes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a prediction and scoring on the binned validation and / or test data\n",
    "predicted = id3.predict(binned_X_test)\n",
    "print(binned_X_test, predicted) # OBS: printing does not make sense with the \"real data\"\n",
    "\n",
    "id3_score(predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualising in the \"bubble\" format from the lecture\n",
    "dot_data = id3.makeDotData().source\n",
    "graph = graphviz.Source(dot_data, format=\"png\")\n",
    "graph.render(test_case+\"_bubbles\")\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing to squares if you want a tree that looks more like those from part 1 ;-)\n",
    "dot_data_pretty='digraph Tree {\\n'+\\\n",
    "    'node [shape=box'+\\\n",
    "    ', style=\"rounded\", color=\"black\"'+\\\n",
    "    ', fontname=\"helvetica\"] ;\\n'+\\\n",
    "    'graph [ranksep=equally, splines=polyline] ;\\n'+\\\n",
    "    'edge [fontname=\"helvetica\"] ;\\n'+\\\n",
    "    dot_data[9:]\n",
    "\n",
    "graph = graphviz.Source(dot_data_pretty, format=\"png\")\n",
    "graph.render(test_case+\"_pretty\")\n",
    "#graph.view()\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Your own random forest!\n",
    "Implement your own random forest (with at least N=5 trees) based on the ID3-implementation you worked with above. Use the best performing data binning from your experiments. As a reminder, the steps for the random forest creation are:\n",
    "- bootstrap N new datasets from the binned *training* data you created above, sets for validation and test form already the remaining out-of-bootstrap data\n",
    "- randomise the set of used attributes for each tree (clarification: do the randomisation per tree only, not for each split within the trees)\n",
    "- create the N trees\n",
    "- use an ensemble to get the final prediction (e.g. *weighted averaging based on the score for each tree*, for example - NOTE: remember that scores can be negative!)\n",
    "- evaluate on the out-of-bootstrap data\n",
    "\n",
    "Motivate your choices and prepare for explaining your implementation in detail!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- IMPLEMENTATION TASK BELOW THIS LINE ------------------------  \n",
    "# Implement a random forest based on the ID3 implementation for a regression tree. \n",
    "# Evaluate in a fair manner (explain your choice!) against the SKLearn Random Forest you created \n",
    "# in Part 1 of the assignment.\n",
    "# ...\n",
    "\n",
    "# dummy values, so that the visualisation works, see below\n",
    "number_of_trees = 2\n",
    "trees_in_forest = [id3, id3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One way of visualising your forest - there might be smarter versions!\n",
    "# OBS: this code assumes number_of_trees and a list with the trees called trees_in_forest\n",
    "# adapt if needed!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "graph = []\n",
    "img_path = []\n",
    "\n",
    "for i in range(number_of_trees):\n",
    "    graph.append(graphviz.Source(trees_in_forest[i].makeDotData().source, format=\"png\"))\n",
    "    img_path.append(\"forest_\"+test_case+str(i))\n",
    "    graph[i].render(img_path[i])\n",
    "    \n",
    "fig, axs = plt.subplots(number_of_trees,1,figsize=(100,100)) # use plt.subplots(number_of_trees/2,2) if you want two columns\n",
    "for i, axi in enumerate(axs.flat):\n",
    "    axi.set_title(\"Tree {}\".format(i))\n",
    "    \n",
    "    img = mpimg.imread(img_path[i]+\".png\")\n",
    "    axi.imshow(img)\n",
    "fig.tight_layout()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "63f08e0be87bb1ec22e3a665002567369c2bb78585d8d1135c35fb08381ea5a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
