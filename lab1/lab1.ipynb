{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background and Tools\n",
    "There are two algorithms with similar names, which makes them prone to confusion although they have different purposes and follow different learning paradigms. One is in fact a clustering algorithm, k-Means, which in an unsupervised manner finds k clusters in a data set. This is done by iteratively setting out / moving k cluster centers and grouping the points in the set closest into the cluster, while at the same time optimising the separation between the groups (clusters) of data points. The other is a classifier, k-NN, which in its basic conceptual form does not actually do anything to fit a model to the given data, it simply finds the k nearest neighbours in the entire data set to an unseen data point, given some distance measure, and assigns it the class of the majority of the k samples. In actual implementations, other techniques are used to structure the data somewhat, so that the search for the closest points only has to be carried out in a part of the data set. However, it can be called supervised, as it is given the class labels of the data points it works with to do this pre-processing.\n",
    "\n",
    "You will in this assignment work with a simplified version of the MNIST handwritten numbers dataset provided by SciKitLearn (sklearn.datasets.digits) and inspect and modify it with UMAP, calculate the cosine similarity of each data sample against the means over the different classes, and later both cluster it with k-Means as well as use the k-NN classifier on it, to then run different evaluation tools. The idea is to have used UMAP (as one example of a tool for dimensionality reduction), done a matrix multiplication in Python / Numpy, understand the different elements of the SciKitLearn confusion matrix and evaluation reports, as well as to see what you can do to evaluate a clustering approach (which is not as naturally done as for a classification, where an answer is either wrong or correct). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Getting started\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1\n",
    "Make sure to have all the necessary basic tools running with the [\"Lab 0: Python intro\"](https://canvas.education.lu.se/courses/32297/pages/lab-0-python-intro) exercise. If not already included in your setup, install UMAP (e.g. with ```pip install umap-learn```, see https://umap-learn.readthedocs.io/en/latest/basic_usage.html for some hints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install with pip install scikit-learn pandas seaborn umap-learn IProgress\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets, metrics\n",
    "from collections import Counter # For majority counting, but is not strictly necessary\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2\n",
    "Load the digits dataset from the datasets provided in SciKitLearn. [The python tutorial by Dennis Medved](Python_introduction.ipynb) can provide you with code snippets and inspiration to this and later parts of the assignment.\n",
    "\n",
    "Inspect the data, plot some sample images (use matplotlib) and be prepared to answer questions about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.3\n",
    "Create and train (fit) a UMAP-reducer, then transform and visualise your data, e.g. doing something like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Example code ###########\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "reducer.fit(digits.data)\n",
    "embedding = reducer.transform(digits.data)\n",
    "\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=digits.target, cmap='Spectral', s=5)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title('UMAP projection of the Digits dataset', fontsize=24);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Be prepared to explain, at least on a conceptual level, what it does__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Calculate the cosine-similarity (should be explained in lecture 2) of each sample against the means of the classes in the data set as following:\n",
    "\n",
    "### Exercise 2.1\n",
    "Calculate the \"mean image\" per class, ```mean_images``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2\n",
    "Calculate the row-wise L2-norm for the raw data, i.e. the L2-norm over each image (check for example Numpy's linalg.norm for that) __and__ for the mean images you got from step the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3\n",
    "Calculate the cosine similarity matrix, ```similarities``` of the normalised raw data and mean image matrices. See https://en.wikipedia.org/wiki/Cosine_similarity. as a reminder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4\n",
    "Plot the closest and \"furthest\" sample (images) for each class, i.e. where cosine similarity is highest / lowest together with the respective mean image. You can use the following plot routine as inspiration to get the images out with some meta data (```similarities``` is the matrix with all the cosine similarities):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "\n",
    "    min_idx = np.argmin(similarities[:,i])\n",
    "    max_idx = np.argmax(similarities[:,i])\n",
    "\n",
    "    fig, axs = plt.subplots(1,3,figsize=(10,10))\n",
    "    \n",
    "    # reference image\n",
    "    axs[0].set_title(f\"{i}\")        \n",
    "    axs[0].imshow(mean_images[i].reshape(8,8))\n",
    "    \n",
    "    # image and data with highest similarity to reference\n",
    "    axs[1].set_title(f\"{max_idx}, sim:{similarities[max_idx, i]:.3f}, y:{digits.target[max_idx]}\")\n",
    "    axs[1].imshow(X[max_idx].reshape(8,8))\n",
    "    \n",
    "    # image and data with lowest similarity to reference\n",
    "    axs[2].set_title(f\"{min_idx}, sim:{similarities[min_idx, i]:.3f}, y:{digits.target[min_idx]}\")\n",
    "    axs[2].imshow(X[min_idx].reshape(8,8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Be prepared to explain__ what you see and reflect upon it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "Split your data set into 70% training data (features and labels), and 30% test data (this will be used for the classifiers below). Use the method ```train_test_split``` from ```klearn.model_selection```. Set the ```random_state=13  ``` and ```stratify``` on the labels (so that the test set has the same number of each label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: k-NN\n",
    "Process the data with a k-NN classifier as follows: \n",
    "\n",
    "### Exercise 4.1\n",
    "Set up a ```sklearn.neighbors.KNeighborsClassifier``` as it comes in SciKitLearn with ```n_neighbors = 5, algorithm = 'brute'``` and otherwise default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1\n",
    "Prepare the classifier (\"fit a model\") with your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2\n",
    " Apply your classifier to the test data, i.e., get predictions for the test data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the nearest neighbors and the corresponding distances in the training set for each test sample  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usage <distances, neighbors> = <classifier>.kneighbors( <test_features>) \n",
    "\n",
    "# *** Your code ***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the 5 nearest neighbors for a few test images (see tutorial)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3\n",
    "Evaluate your classifier with the sklearn.metrics tools classification_report and confusion_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage metrics.classification_report( <test_labels>, <predicted_labels>) \n",
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USage metrics.confusion_matrix( <test_labels, <predicted_labels>)\n",
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: k-Means\n",
    "Cluster the data with k-Means as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1\n",
    "Set up a k-Means instance (```sklearn.cluster.KMeans```) with ```n_clusters=10```, ```n_init=100``` and ```random_state=42```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*** Your code ***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2\n",
    "Apply the clustering approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<clusters> = <clustering>.fit(<train_features>)\n",
    "# *** Your code ***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.3\n",
    "Visualise the found cluster centers (you can get them using <clustering>.cluster_centers_); those are in principle also images, but maybe not as clearly interpretable as you would have hoped for. Compare what you see with the scatter plot you got from UMAP, i.e. find k-Means-clusters (visualised as cluster center images) and corresponding UMAP-plot-clusters (visualised as blobs or scattered dots in the plot) that are clearly distinct from everything else or others that seem to be \"all the same\" or at least very close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.4\n",
    "Investigate at least the following evaluation tools in the SciKitLearn documentation: ```metrics.completeness_score( <labels>, <cluster-labels>)``` and \n",
    "```metrics.homogeneity_score( <labels>, <cluster-labels>)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those to work you should run a prediction step over the training data (into cluster-labels). Rough numbers for what those measures should be can be found in the check list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the interested: Try also the following score (the main concept is the mutual information score, the adjusted mutual information score is an improved variant) and give its documentation some thoughts, discuss it with your partner and in the peer review (it should make more sense after the information theory lecture, but there is also some intuition in it, that you can reflect upon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage metrics.adjusted_mutual_info_score( <labels>, <cluster-labels>)\n",
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.5\n",
    "For each cluster, assign a label by taking the most common label of the training data assigned to the cluster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************\n",
    "# Assign labels to clusters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.6\n",
    "Use the clustering result from the previous exercise to do a prediction for the test data and evaluate the result. It should be quite bad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************\n",
    "# *** Your code ***\n",
    "# *****************\n",
    "# Predict on the test set\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
